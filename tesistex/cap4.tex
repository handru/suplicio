\label{pagcap4}
\chapter{Experimentación y Análisis de Resultados}

\section{Introducción}

En este capitulo se presentan los resultados obtenidos en cada etapa de la optimización, especialmente el speedup de la aplicación así como la utilización de recursos. Se mostrará también el estado del sistema durante las distintas ejecuciones de la aplicación.

Debido a que la optimización se realizó en varios pasos se mostrarán los resultados iniciales, parciales y finales del proceso. De esta manera es posible ver el impacto de cada parte de la optimización en el código legacy. Como indican \citep{Grama} la optimización previa del código serial es necesaria para evitar efectos indeseados en las mediciones, y puede representar un factor de aceleración de la aplicación de entre 2X y 5X, es decir, dos a cinco veces más rápido. 
La aplicación fue modificada lo menos posible en el proceso de optimización por lo cual no se alcanza toda la mejora posible en una recodificación, pero como se explicó anteriormente, se trató de hacer los cambios lo mas transparente posibles al usuario y creador de la aplicación.
El código de la aplicación objeto de estudio de esta tesis es entregado junto con los resultados generados para dos conjuntos de datos de entrada, uno para un problema definido de una paleta dividida en 50 lineas de 50 paneles, de ahora en más un problema de tamaño 50x50, y otra para uno de 80 lineas de 80 paneles, problema de tamaño 80x80, siendo estos valores definidos en un archivo que sirve de entrada de datos para la aplicación. 

Para este trabajo de tesis se elige trabajar principalmente con el conjunto de datos resultante del caso de cantidad de paneles 50x50, sin embargo se presentan observaciones obtenidas de una prueba en uno de los equipos para el caso de tamaño 80x80. El usuario y creador de la aplicación indicó que la ejecución de la aplicación con el problema de tamaño de 50x50 paneles demoraba en el orden de horas de ejecución. El problema de tamaño de 80x80 se dejaba ejecutando de un día para el otro. Como se explicó anteriormente, no hay datos de las ejecuciones del usuario, por lo cual se toman ejecuciones del código original en las arquitecturas de prueba para referencia.

\section{Arquitecturas de prueba}

Las pruebas se llevaron a cabo en dos equipos para obtener resultados que permitieran realizar una mejor evaluación del proceso de optimización. 
Las computadoras utilizadas fueron una PC y una Notebook, ambas multiprocesador y con arquitectura de 64 bits. A continuación la descripción de los equipos: 
\begin{itemize}
\item Equipo 1 (PC Clon):
     \begin{itemize}
      \item Procesador AMD Phenom II x4 955 x86\_64
            \begin{itemize}
             \item 4 núcleos reales.
             \item Frecuencia máxima de 3.2 Ghz.
             \item Release date: Abril del 2009.
            \end{itemize}
      \item Mother ASUS M4A785TD-V EVO
      \item 4Gb RAM DDR3 1333Mhz.
      \item HD SATA II 3Gbps.
      \item USB 2.0 (480 Mbps)
     \end{itemize}
\end{itemize}
\begin{itemize}
\item Equipo 2 (Notebook):
      \begin{itemize}      
      \item Procesador Intel Core i3-370M x86\_64
             \begin{itemize}
             \item 2 núcleos reales + 2 hilos de control por núcleo.
             \item Frecuencia máxima de 2.4 Ghz
             \item Release date: Junio del 2010.
             \end{itemize}
      \item Mother Dell 0PJTXT-A11.
      \item 6Gb RAM DDR3 1333Mhz.
      \item HD SATA II 3Gbps.
      \item USB 2.0 (480 Mbps)
      \end{itemize}
\end{itemize}

Nos referiremos en adelante al primer equipo como PC1 y al segundo equipo como PC2.
Se utilizó una versión Live USB de Slackware Linux como sistema operativo para las pruebas. Como disco de almacenamiento sobre el que corría la aplicación se utilizó un Flash Drive USB, en el cual se crearon los archivos durante la ejecución.

Una nota sobre la arquitectura del procesador de PC2. En este caso el procesador tiene dos núcleos, pero al ofrecer dos hilos de control por núcleo, el sistema operativo los ve como si tuviera disponibles cuatro núcleos. El procesador luego distribuye los recursos disponibles sobre cada hilo de acuerdo a lo solicitado por el sistema operativo.

\section{Mediciones}
Para las mediciones de tiempo se utilizó el comando \emph{time}\footnote{Para una referencia del comando en GNU/Linux ver su manual: ``man 1 time''.} de manera de poder evaluar el tiempo real consumido por la aplicación en las diferentes etapas del trabajo de tesis: programa original, optimizado serialmente, optimizado paralelamente. Mostraremos los tiempos en los equipos seleccionados para las pruebas y las mejoras en desempeño que obtuvimos en el programa en cada iteración de la optimización. Para ambos equipos se realizaron mediciones del tamaño de problema de 50x50, y para el tamaño de problema de 80x80 se utilizó el equipo PC2. Realizamos pruebas con ambos tamaños de datos para poder determinar la escalabilidad de la solución aplicada, además de poder verificar como impacta en el equipo el cambio de tamaño del problema.

Para el tamaño de problema de 80x80 paneles, como se indicó en el capítulo 3, el archivo con los datos de entrada para la ejecución de la aplicación, \emph{entvis2f.in}, posee una única modificación con respecto al mismo archivo para el tamaño de problema de 50x50, se define $nr = 80$ y $no = 80$. 

Luego mediante el análisis de las diferencias entre los códigos de la versión de tamaño 50x50 contra la de 80x80, observamos que el código en los bloques ``common'' de Fortran indica lo siguiente:

\noindent Para el caso 50x50
\begin{lstlisting}[style=For, numbers=none]
   parameter (maxir=51,maxio=51,...
\end{lstlisting}
Para el caso 80x80
\begin{lstlisting}[style=For, numbers=none]
   parameter (maxir=81,maxio=81,...
\end{lstlisting}
Como indicamos en el capítulo 3, \emph{maxir} y \emph{maxio} son lo mismo que nr+1 o no+1, lo cual sería una manera más simple de definirlo. Debido a que la definición de estos valores está fija, literalmente, en cada bloque common de todo el código, es que para las optimizaciones, serial y paralela, de la aplicación con tamaño de problema 80x80, se debe cambiar en todo el código cada una de las definiciones de \emph{maxir} y \emph{maxio}.

Luego de adaptado esto se puede compilar cada versión de la aplicación para el tamaño de problema de 80x80 de la misma manera que la versión de tamaño 50x50.

También se incluyen muestras del estado de los archivos en disco luego de la ejecución del programa, el estado de la memoria y la CPU en plena ejecución del programa, para mostrar los resultados de las optimizaciones realizadas.

\subsection{Estado inicial y primeras mediciones}
Lo primero que hicimos fue compilar y ejecutar el programa original para calcular el tiempo inicial de referencia para el resto del trabajo, resguardando de una posible reescritura a los datos originales, que luego utilizaremos para poder verificar la correctitud de las distintas versiones del proceso de optimización. Acerca de esto, lo que se realizó fue una comparación de los resultados producidos en los archivos de salida de cada versión de la aplicación con los originales obtenidos por el usuario, verificando que sean exactamente los mismos.

En ambos equipos realizamos la compilación con el siguiente comando:
\begin{lstlisting}[style=consola, numbers=none]
   $ gfortran -o serial invisidos2fin.for
\end{lstlisting}
Esto crea un archivo ejecutable llamado ``serial''.
Para poder lanzar el ejecutable y poder verificar el tiempo lo realizamos con el comando:
\begin{lstlisting}[style=consola, numbers=none]
   $ time ./serial
\end{lstlisting}

\subsubsection{Tiempos}
En la Fig. \ref{figTSerial} se puede observar el tiempo resultante calculado por el comando \emph{time}, donde se obtiene un tiempo total de ejecución (linea ``real'') para el tamaño 50x50 en PC1 de 21 min. 48 seg. y en PC2 de 22 min. 56 seg. 

Para la versión de tamaño 80x80 podemos observar también en la Fig. \ref{figTSerial} que la ejecución en el equipo PC2 indica un tiempo de ejecución de 225 min. 43 seg., es decir 3 hs. 45 min. 43 seg. El tamaño del problema se incrementa de 2500 paneles a 6400 paneles, un incremento de factor 2.56 veces, pero el tiempo se hace exponencial, en un factor de 9.78.

%\begin{figure}[htb]
% \centering
%	\begin{lstlisting}[style=consola1, numbers=none]
%   real    225m43.721s
%    user    174m29.803s
%    sys     3m11.953s
%    live@PC1 $ 
%	\end{lstlisting}
%\caption{Tiempo de la versión original con tamaño 80x80.}
%\label{figTS80}
%\end{figure}

Podemos observar que con un cambio en la arquitectura del procesador (PC1 con 4 núcleos reales, PC2 con 2 núcleos y 2 hilos de control por núcleo) se incurre en una demora de 1m8s. Se tomó otra muestra con el equipo PC2 y se obtuvo un resultado similar, 23 min. 1 seg. por lo que podríamos indicar que la diferencia persiste y se mantiene dentro de un margen de tiempo. Esta diferencia observada se debe posiblemente a la mayor velocidad del procesador en PC1 o al mayor gasto (\emph{overhead}) de tiempo en la administración y comunicación de los hilos por la arquitectura SMT de PC2. También sería de interés investigar el uso de la jerarquía de memoria, especialmente de las caches, en ambos procesadores.

Como el programa es serial, siempre utilizó en su ejecución el mismo núcleo o hilo de ejecución. En la Fig. \ref{figTopS} podemos observar una captura del comando \emph{top} en PC1, donde se puede ver la aplicación original en ejecución sobre la CPU2.

\begin{figure}[t!]
 \centering
    \begin{subfigure}[t]{0.4\linewidth}
    \centering
		\begin{lstlisting}[style=consola, numbers=none]
    real    21m48.109s 
    user    19m3.067s
    sys     0m29.685s
    live@PC1 $ 
		\end{lstlisting}
	\caption{Equipo PC1}
	\end{subfigure}%
	\hspace{.5cm}%
    \begin{subfigure}[t]{0.4\linewidth}
    \centering
		\begin{lstlisting}[style=consola, numbers=none]
    real    22m56.392s 
    user    20m7.858s
    sys     0m32.917s
    live@PC2 $ 
		\end{lstlisting}
	\caption{Equipo PC2}
	\end{subfigure}
    \begin{subfigure}[t]{0.4\linewidth}
    \centering
		\begin{lstlisting}[style=consola, numbers=none]
    real    225m43.721s
    user    174m29.803s
    sys     3m11.953s
    live@PC2 $ 
		\end{lstlisting}
	\caption{Equipo PC2 - Tamaño 80x80}
	\end{subfigure}%
\caption{Tiempos de la versión serial original.}
\label{figTSerial}
\end{figure}

\begin{figure}[htb]%[htp]
  \centering
  \includegraphics[width=0.80\textwidth]{figuras/clon-top-serial.png} \\
  \caption{Comando \emph{top}: Aplicación original en subrutina \emph{estela}.} 
   \label{figTopS}
\end{figure}

\subsubsection{Archivos en disco}
La ejecución genera para ambos tamaños de problema (50x50 y 80x80 paneles) todos los archivos utilizados para cálculos intermedios y resultados finales así como los temporales con los que el programa trabaja. 

La ejecución serial del programa original generó la misma cantidad de archivos, 58 archivos (Fig. \ref{figListS}) entre los ``.txt'', ``.plt'', ``.out'' y los ``.tmp'', esto es así por el determinismo del programa. No contamos el archivo ejecutable ni el de datos de ingreso ``entvis2f.in''. 

El tamaño en disco ocupado si difiere entre los tamaños de problema. Como podemos observar en la Fig. \ref{figListS}, para el tamaño de 50x50, tanto en PC1 como en PC2 el tamaño de los archivos fue de 684 Mb, donde el mayor tamaño era ocupado por los ocho archivos ``.tmp'', de los cuales siete ocupan 96 Mb cada uno para un total de 672 Mb. 

Para el tamaño de 80x80 el espacio en disco utilizado fue de 4415Mb o 4.3GB, siendo los archivos ``.tmp'' los que ocupaban 4375MB, siete de los ocho archivos pesando 625MB cada uno.

\begin{figure}[htb]%[htp]
\centering
  \begin{subfigure}[t]{1\linewidth}
  \centering
  \includegraphics[width=0.80\textwidth]{figuras/clon-serial-arch-du.png} \\
  \caption{PC1}
  \end{subfigure}
  \begin{subfigure}[b]{1\linewidth}
  \centering
  \includegraphics[width=0.80\textwidth]{figuras/dell-serial-arch-du.png} \\
  \caption{PC2}
  \end{subfigure}
\caption{Problema tamaño 50x50 - Original: Lista de archivos y tamaño del directorio por equipo.} 
\label{figListS}
\end{figure}

\subsubsection{Memoria RAM}
La cantidad de memoria consumida por la aplicación para el tamaño de problema de 50x50 al iniciar en cada equipo es de 217 MB en PC1 y lo mismo en PC2. Cuando durante la ejecución la aplicación ingresa en la subrutina \emph{solgauss} la memoria se incrementa a 255 MB. Y al salir de esta subrutina la memoria baja a 217 MB nuevamente. La salida por pantalla de la aplicación nos permite saber en que subrutina se encuentra, por ello en tiempo de ejecución podemos determinar el estado de la memoria para el proceso. Justamente la rutina \emph{solgauss} representa el máximo en la cantidad de memoria consumida por la aplicación.

Para el tamaño 80x80, los datos observados muestran que en memoria RAM la aplicación llega a ocupar 1293 Mb o 1.26 GB fuera de la subrutina \emph{solgauss} y 1581 Mb dentro de la subrutina.

Estos datos se obtienen del comando \emph{pmap}\footnote{Para una referencia del comando en GNU/Linux ver su manual: ``man pmap''.} aplicado sobre el proceso en ejecución, por ejemplo si la aplicación tiene PID 2228:
\begin{lstlisting}[style=consola, numbers=none]
   $ pmap -x 2228
\end{lstlisting}

\emph{Pmap} reporta información del mapa de memoria de un proceso, dando en su última línea un total en Kbytes de la memoria utilizada, importándonos la primer columna donde indica el total de memoria utilizada por el proceso. Por ejemplo en la Fig. \ref{figPmap} vemos el resultado para cada equipo mientras se ejecutaba la aplicación original para el tamaño de problema de 50x50. El comando ``top'' también permite observar el mismo valor que indica ``pmap'' en su columna VIRT.

\begin{figure}[htb]
    \begin{subfigure}[t]{0.5\linewidth}
    \centering
	\begin{lstlisting}[style=consola, numbers=none]
[datos de la aplicación]
------------- ------- ------- -------
total kB      222212  157080  154368
	\end{lstlisting}
	\caption{Equipo PC1}
	\end{subfigure}%
	\hspace{.5cm}%
    \begin{subfigure}[t]{0.5\linewidth}
    \centering
	\begin{lstlisting}[style=consola, numbers=none]
[datos de la aplicación]
------------- ------- ------- -------
total kB      222472  209160  206240
	\end{lstlisting}
	\caption{Equipo PC2}
	\end{subfigure}
\caption{Información del comando \emph{pmap} en cada equipo.}
\label{figPmap}
\end{figure}

En la tabla \ref{tab:tabDataSerial} se muestran los datos recopilados hasta el momento en el trabajo de tesis, todos de la aplicación serial original. Las dos subsecciones siguientes mostrarán como evolucionó con la optimización, tomando como base para comparación los tiempos y tamaños obtenidos en esta primer etapa. 

\begin{table}[htb]
\begin{center}
\begin{tabular}{|l|l|c|c|c|} 
\hline
\multicolumn{2}{|l|}{\rule[-1ex]{0pt}{2.5ex} Tamaños de problema} & \multicolumn{2}{c|}{tamaño 50x50} & tamaño 80x80 \\ 
\hline 
\hline
%\rule[-1ex]{0pt}{2.5ex} 
\multicolumn{2}{|l|}{\rule[-1ex]{0pt}{2.5ex} Equipos} & PC1 & PC2 & PC2 \\ 
\hline 
\multicolumn{2}{|l|}{\rule[-1ex]{0pt}{2.5ex} Archivos generados} & 58 & 58 & 58\\
\hline
\multicolumn{2}{|l|}{\rule[-1ex]{0pt}{2.5ex} Esp. en disco utilizado} & 684Mb & 684Mb & 4415Mb\\
\hline
\rule[-1ex]{0pt}{2.5ex} \multirow{2}{3cm}{Memoria} & Ejecución en \emph{solgauss} & 255Mb & 255Mb & 1581Mb\\ \cline{2-5}
& Resto del programa & 217Mb & 217Mb & 1293Mb\\ \cline{2-5}
\hline
\multicolumn{2}{|l|}{\rule[-1ex]{0pt}{2.5ex} Hilos de ejecución utilizados} & 1 & 1 & 1\\
\hline
\multicolumn{2}{|l|}{\rule[-1ex]{0pt}{2.5ex} Tiempo total de ejecución} & 21 min 48 seg & 22 min 56 seg & 3 hs 45 min 43 seg\\
\hline
\end{tabular} 
\caption {Datos de ejecución de la aplicación serial original.}
\label{tab:tabDataSerial}
\end{center}
\end{table}

\subsection{Optimización serial y mediciones intermedias}
Luego de realizar la optimización serial se tomaron nuevamente mediciones. La compilación se realizó con el mismo comando ya que la aplicación sigue siendo serial y por lo tanto no hay adiciones de paralelización a la computación. La nueva versión de la aplicación la denominamos \emph{optserial}.
en esta etapa aún no tenemos la adición de ninguna optimización paralela.
\begin{lstlisting}[style=consola, numbers=none]
      $ gfortran -o optserial invisidos2fin_optSerial.for
\end{lstlisting}
Y nuevamente para medir el tiempo del programa ejecutamos la aplicación con la instrucción \emph{time}.
\begin{lstlisting}[style=consola, numbers=none]
      $ time ./optserial
\end{lstlisting}

Para comparar los tiempos entre las versiones de la aplicación nos interesa calcular el \emph{factor de mejora del rendimiento} (\emph{speed-up}, rapidez, aceleración), el cual se define por la ecuación \ref{eq2}:

\begin{equation}\label{eq2}
S(n) = T(1)/T(n)
\end{equation}

Donde $S(n)$ es el \emph{speed-up}, $T(1)$ es el tiempo de ejecución en un procesador y $T(n)$ es el tiempo de ejecución en \emph{n} procesadores. Para este caso de optimización serial tomaremos $T(n)$ como el tiempo de ejecución de la nueva versión del programa.

Mas información sobre este tema se puede encontrar en \citep{Hwang}.

\subsubsection{Tiempos}
En la Fig. \ref{figTIfiles} se pueden observar los tiempos obtenidos para ambos tamaños de  problema. Para el problema de tamaño 50x50 el tiempo obtenido para \emph{optserial} en PC1 fue de 16 min 2 seg, lo que representa una disminución en el tiempo de 5 min 36 seg aproximadamente sobre la versión serial original de la aplicación en el mismo equipo. Aplicando la ecuación \ref{eq2} de \emph{speed-up} se tiene un factor de mejora del rendimiento de 1.35.

En la computadora PC2 para el tamaño 50x50 los tiempos obtenidos fueron de 17 min 4 seg. Tenemos una disminución con respecto a la versión serial original de 5 min 52 seg. Aplicando nuevamente la ecuación \ref{eq2} resulta en una mejora de factor 1.34.

Se puede ver que el factor de mejora alcanzado entre el original serial y el optimizado es muy similar entre ambos equipos, con una diferencia de solo 0.01, y que es levemente mejor en PC1.

Para el tamaño de problema de 80x80 los tiempos observados en la versión optimizada del código serial son de 150 min. 45 seg., es decir 2 hs. 30 min 45 seg. Con respecto a la versión de tamaño 50x50, el tiempo de ejecución se incrementa en 8.83 veces, menos que la diferencia entre los tiempos de las versiones originales de la aplicación (9.78 veces).
De esto podemos determinar que el \emph{speed-up} será mayor también que para el tamaño de problema menor. Observamos una ganancia de tiempo con respecto a la aplicación serial de 75 min. aproximadamente, que al calcular \ref{eq2} nos da un factor de 1.5. 

Esta mejora puede explicarse por la mayor cantidad de datos en disco que utiliza la aplicación en el tamaño 80x80 y que ahora son accedidos en memoria.

Nuevamente en el caso de la CPU podemos observar que un solo procesador es el encargado de realizar la tarea ya que aún no se optimiza paralelamente. En la Fig. \ref{figTopI} podemos observar como ejemplo, la ejecución de la aplicación optimizada serialmente en PC1, en el momento que está dentro de la subrutina \emph{estela}.

\begin{figure}[htb] %[t!]
 \centering
    \begin{subfigure}[t]{0.4\linewidth}
    \centering
		\begin{lstlisting}[style=consola, numbers=none]
    real    16m2.124s
    user    16m0.894s
    sys     0m0.259s
    live@PC1 $ 
		\end{lstlisting}
	\caption{Equipo PC1}
	\end{subfigure}%
	\hspace{.5cm}%
    \begin{subfigure}[t]{0.4\linewidth}
    \centering
		\begin{lstlisting}[style=consola, numbers=none]
    real    17m4.161s
    user    17m2.631s
    sys     0m0.428s
    live@PC2 $ 
		\end{lstlisting}
	\caption{Equipo PC2}
	\end{subfigure}
    \begin{subfigure}[t]{0.4\linewidth}
    \centering
		\begin{lstlisting}[style=consola,numbers=none]
    real    150m45.602s
    user    150m36.178s
    sys     0m3.413s
    live@PC1 $ 
		\end{lstlisting}
	\caption{Equipo PC2 - Tamaño 80x80}
	\end{subfigure}
\caption{Tiempo de la versión optimizada serialmente.}
\label{figTIfiles}
\end{figure}

\begin{figure}[htb]%[htp]
  \centering
  \includegraphics[width=0.80\textwidth]{figuras/clon-top-ifiles.png} \\
  \caption{Comando \emph{top}: Aplicación opt. serialmente en subrutina \emph{estela}.} 
   \label{figTopI}
\end{figure}

\subsubsection{Archivos en disco}
En la Fig. \ref{figListI} observamos que luego de la optimización serial han desaparecido del directorio los archivos ``.tmp'', esto se debe a que los cálculos intermedios ahora son almacenados en memoria RAM. Esto ocurre tanto para el tamaño de 50x50 paneles como el de 80x80. El resto de archivos (50 en total) siguen creándose, pero al demorar la escritura de los archivos utilizados para ir mostrando y almacenando la salida por pantalla, tanto como los que son leidos y escritos y obtienen resultados finales, se logra evitar el acceso constante al disco a través de la ejecución de la aplicación, para tener sólo que hacerlo una vez por archivo al finalizar la ejecución del programa o una subrutina en particular.

El tamaño ocupado por los archivos de la aplicación fue de 17 Mb para el tamaño 50x50, tanto en PC1 como en PC2, y de 40 Mb para el tamaño 80x80, lo podemos ver en la Fig. \ref{figListI}. De estos datos se puede observar el impacto de no generar los archivos ``.tmp'' en disco.

\begin{figure}[h]%[htp]
  \centering
  \begin{subfigure}[t]{1\linewidth}
  \centering
  \includegraphics[width=0.80\textwidth]{figuras/clon-ifiles-arch-du.png} \\
  \caption{PC1}
  \end{subfigure}
  \begin{subfigure}[b]{1\linewidth}
  \centering
  \includegraphics[width=0.80\textwidth]{figuras/dell-ifiles-arch-du.png} \\
  \caption{PC2}
  \end{subfigure}
\caption{Problema tamaño 50x50 - Opt. Serial: Lista de archivos y tamaño del directorio por equipo.} 
\label{figListI}
\end{figure}

\subsubsection{Memoria RAM}
Observando la memoria en esta versión de la aplicación para el tamaño 50x50, obtenemos que consume 552 Mb mientras está en \emph{solgauss} y 504MB el resto de la ejecución, tanto en PC1 como PC2, esto se puede ver en la Fig. \ref{figPmapI}. Esto significa un incremento en la cantidad de memoria utilizada, en esta versión optimizada serialmente con respecto a la versión serial original, de 297MB cuando el programa está en la subrutina \emph{solgauss} y de 287MB antes o después de dicha subrutina. Este incremento se debe a los archivos ``.tmp'' que ya no utiliza mas en disco y debe llevar en memoria como internal files. 

En el tamaño de 80x80 también observamos un incremento en la memoria. Observamos que en ejecución la aplicación utiliza mientras está en \emph{solgauss} 3483MB (3.4GB) y 3171MB (3.09GB) en el resto de la ejecución. El equipo cuenta con 6GB de memoria RAM por lo que no fue necesario que realizara intercambio hacia disco (\emph{swapping}), lo que hubiera impactado en los tiempos.

\begin{figure}[htb]
    \begin{subfigure}[t]{0.5\linewidth}
    \centering
	\begin{lstlisting}[style=consola, numbers=none]
[datos de la aplicación]
------------- ------- ------- -------
total kB      516392  504060  501276
	\end{lstlisting}
	\caption{Equipo PC1}
	\end{subfigure}%
	\hspace{.5cm}%
    \begin{subfigure}[t]{0.5\linewidth}
    \centering
	\begin{lstlisting}[style=consola, numbers=none]
[datos de la aplicación]
------------- ------- ------- -------
total kB      516524  504308  501332
	\end{lstlisting}
	\caption{Equipo PC2}
	\end{subfigure}
\caption{Comando \emph{pmap} con la aplicación optimizada serialmente (fuera de \emph{solgauss}).}
\label{figPmapI}
\end{figure}

\begin{table}[htb]
\begin{center}
\begin{tabular}{|l|l|c|c|c|} 
\hline
\multicolumn{2}{|l|}{\rule[-1ex]{0pt}{2.5ex} Tamaños de problema} & \multicolumn{2}{c|}{tamaño 50x50} & tamaño 80x80 \\ 
\hline 
\hline
%\rule[-1ex]{0pt}{2.5ex} 
\multicolumn{2}{|l|}{\rule[-1ex]{0pt}{2.5ex} Equipos} & PC1 & PC2 & PC2 \\ 
\hline 
\multicolumn{2}{|l|}{\rule[-1ex]{0pt}{2.5ex} Archivos generados} & 50 & 50 & 50\\
\hline
\multicolumn{2}{|l|}{\rule[-1ex]{0pt}{2.5ex} Esp. en disco utilizado} & 17Mb & 17Mb & 40Mb\\
\hline
\rule[-1ex]{0pt}{2.5ex} \multirow{2}{3cm}{Memoria} & Ejecución en \emph{solgauss} & 552Mb & 552Mb & 3483Mb\\ \cline{2-5}
& Resto del programa & 504Mb & 504Mb & 3171Mb\\ \cline{2-5}
\hline
\multicolumn{2}{|l|}{\rule[-1ex]{0pt}{2.5ex} Hilos de ejecución utilizados} & 1 & 1 & 1\\
\hline
\multicolumn{2}{|l|}{\rule[-1ex]{0pt}{2.5ex} Tiempo total de ejecución} & 16 min 2 seg & 17 min 4 seg & 2 hs 30 min 45 seg\\
\hline
\end{tabular} 
\caption {Datos de ejecución de la aplicación optimizada serialmente.}
\label{tab:tabDataIfiles}
\end{center}
\end{table}

La Tabla \ref{tab:tabDataIfiles} resume la información obtenida de la optimización serial de la aplicación. En la siguiente sección se ven los resultados de la optimización paralela mediante OpenMP.

\subsection{Optimización Paralela y mediciones finales}

Finalmente realizamos las pruebas con la versión optimizada paralelamente del programa. Para esta prueba cambiamos la forma de compilar el programa ya que se debe indicar que aprovechará las directivas de OpenMP, esto lo realizamos pasando el parámetro ``-fopenmp'' al comando de compilación, de la siguiente manera:
\begin{lstlisting}[style=consola, numbers=none]
   $ gfortran -fopenmp -o paralelo invisidos2fin_optOMP.for
\end{lstlisting}
Al terminar tenemos un ejecutable listo para aprovechar la paralelización que brinda OpenMP. 
Nuevamente se ejecutó la aplicación con el comando time, de manera de obtener el tiempo de ejecución. La ejecución se hizo sin limitar la cantidad de threads creados en OpenMP, es decir que la aplicación se ejecutó aprovechando todos los threads disponibles por defecto, es decir uno por cada núcleo disponible (cuatro threads en cada equipo). 
\begin{lstlisting}[style=consola, numbers=none]
   $ time ./paralelo
\end{lstlisting}

Un contratiempo que ocurrió en este paso fue que al ingresar en la parte paralelizada, la aplicación incurrió en un error de ``segmentation fault''. El problema ocurre por el tamaño máximo definido en el kernel Linux de la pila para un proceso, el cual por defecto es de 8192Kb. La solución es previo a la ejecución de la aplicación, definir el tamaño máximo de la pila en ``unlimited'' con el siguiente comando:
\begin{lstlisting}[style=consola, numbers=none]
   $ ulimit -s unlimited
\end{lstlisting}
Luego de establecido el parámetro, la ejecución de la aplicación es correcta.

\subsubsection{Tiempos} 
Los resultados de ``time'' para PC1 indicaron un tiempo de ejecución de 6m5.294s (Fig. \ref{figTOmp}). Al comparar con los 21m48.109s que tomó en su versión original podemos observar 15m42s de mejora aproximada, obteniendo un factor de 3.58 de mejora en el desempeño, lo cual es muy superior a la ganancia inicial con la optimización serial.
En PC2 obtuvimos 8m50.822s de tiempo de ejecución (Fig. \ref{figTOmp}), mientras el programa original tomó 22m56.392s, es decir aproximadamente 14m6s más rápida la versión paralela, obteniendo un factor de 2.59 de mejora en el desempeño.
La diferencia de tiempo de ejecución entre la aplicación optimizada paralelamente en PC1 y PC2 es de 2m45s, observándose esta vez una diferencia de tiempo considerable.
Se podría investigar la incidencia de los 4 núcleos reales del procesador AMD en PC1 contra los 2 núcleos reales y 2 hilos de control por núcleo en el procesador Intel de PC2. Ambos procesadores brindan a OpenMP cuatro hilos, pero los recursos son asignados de manera diferente.

\begin{figure}[htb]
 \centering
    \begin{subfigure}[t]{0.4\linewidth}
    \centering
		\begin{lstlisting}[style=consola, numbers=none]
    real    6m5.294s
    user    17m38.896s
    sys     0m0.872s
    live@PC1 $ 
		\end{lstlisting}
	\caption{Equipo PC1}
	\end{subfigure}%
	\hspace{.5cm}%
    \begin{subfigure}[t]{0.4\linewidth}
    \centering
		\begin{lstlisting}[style=consola, numbers=none]
    real    8m50.822s
    user    28m21.227s
    sys     0m4.812s
    live@PC2 $ 
		\end{lstlisting}
	\caption{Equipo PC2}
	\end{subfigure}
\caption{Tiempo de la versión optimizada paralelamente con OpenMP.}
\label{figTOmp}
\end{figure}

En el consumo de CPU esta vez podemos observar diferencia entre los programas seriales y uno paralelizado. Se han activado todos los núcleos disponibles en el equipo al momento de entrar en la zona de la subrutina \emph{estela} (Fig. \ref{figTopO}), ya sean núcleos reales (PC1) o virtuales (PC2). 
Como ya indicamos, la activación de los núcleos no fue administrada de manera directa con directivas OpenMP por lo cual todos los núcleos disponibles fueron utilizados, pero como se indicaba en el capitulo 2, hay más directivas que pueden ser estudiadas de OpenMP que podrían ser utilizadas para disminuir o incrementar la cantidad de hilos generados en una región paralela y estudiar el impacto y la utilización de los recursos en el multiprocesador.

\begin{figure}[htb]%[htp]
  \centering
  \includegraphics[width=0.80\textwidth]{figuras/clon-top-omp.png} \\
  \caption{Comando \emph{top}: Aplicación optimizada con OpenMP en subrutina \emph{estela}.} 
   \label{figTopO}
\end{figure}

\subsubsection{Archivos en disco}
El directorio de ejecución del programa queda igual que en la versión optimizada serialmente (Ver Fig. \ref{figListI}) ya que en esta nueva versión se han agregado las directivas OpenMP utilizadas y no se ha tocado el código serial ni el tratamiento de los archivos. Lo mismo ocurre con el tamaño ocupado por los archivos en disco (17MB). 

\begin{figure}[htb]
    \begin{subfigure}[t]{0.5\linewidth}
    \centering
	\begin{lstlisting}[style=consola, numbers=none]
[datos de la aplicación]
------------- ------- ------- -------
total kB      480008  455704  452516
	\end{lstlisting}
	\caption{Equipo PC1}
	\end{subfigure}%
	\hspace{.5cm}%
    \begin{subfigure}[t]{0.5\linewidth}
    \centering
	\begin{lstlisting}[style=consola, numbers=none]
[datos de la aplicación]
------------- ------- ------- -------
total kB      480144  455828  452576
	\end{lstlisting}
	\caption{Equipo PC2}
	\end{subfigure}
\caption{Comando \emph{pmap} sobre aplicación optimizada con OpenMP (fuera de \emph{solgauss}).}
\label{figPmapO}
\end{figure}

\subsubsection{Memoria RAM}
Observando la memoria el programa consume 516MB de RAM durante la subrutina \emph{solgauss} y 468MB en el resto de su ejecución en ambos equipos (Fig. \ref{figPmapO}). Con respecto al original esto indica un incremento de 261MB de memoria mientras está en \emph{solgauss} y 251MB en el resto de la ejecución. Al comparar con la aplicación optimizada serialmente se observó que el consumo de memoria es menor en la versión con OpenMP. Ocupa 36MB menos durante la ejecución, tanto si se ejecuta en \emph{solgauss} como en el resto del tiempo. 
Podría investigarse esta diferencias en la memoria en la optimización que realiza el compilador en el código para utilizar las directivas de OpenMP. 

Finalmente podemos ver en la Fig. \ref{figDataTotal} los datos resumidos de las tres versiones de la aplicación.

\begin{figure}[htb]%[htp]
  \centering
  \includegraphics[width=1.0\textwidth]{figuras/tabla-data-final1.png} \\
  \caption{Resumen datos totales de las aplicaciones.} 
   \label{figDataTotal}
\end{figure}

%%%%% hasta aca...
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{comment}
\section{Pruebas con 80x80 paneles}
Para determinar la escalabilidad de la solución aplicada y cómo impacta en un equipo el cambio del tamaño de los datos para el cálculo, se realizaron en PC2 testeos con el segundo tamaño de prueba provisto por el autor/usuario de la aplicación, 80x80 paneles.
Solo tenemos los resultados de salida de la ejecución del original utilizado por el usuario y se utilizaron para verificar los resultados de salida de las versiones utilizadas para la prueba, arrojando que todas dan una salida correcta de los datos.
Como se indicó en el capítulo 3, el archivo con los datos de entrada para la ejecución de la aplicación, \emph{entvis2f.in}, posee una única modificación con respecto al mismo archivo para el tamaño de problema de 50x50, se define nr = 80 y no = 80. Luego mediante el análisis de las diferencias entre los códigos de la versión de tamaño 50x50 contra la de 80x80, observamos que el código en los bloques ``common'' de Fortran indica lo siguiente:

\noindent Para el caso 50x50
\begin{lstlisting}[style=For, numbers=none]
   parameter (maxir=51,maxio=51,...
\end{lstlisting}
Para el caso 80x80
\begin{lstlisting}[style=For, numbers=none]
   parameter (maxir=81,maxio=81,...
\end{lstlisting}
Como indicamos en el capítulo 3, \emph{maxir} y \emph{maxio} son lo mismo que nr+1 o no+1, lo cual sería una manera más simple de definirlo. Debido a que la definición de estos valores está fija, literalmente, en cada bloque common de todo el código, es que para las optimizaciones, serial y paralela, de la aplicación con tamaño de problema 80x80, se debe cambiar en todo el código cada una de las definiciones de \emph{maxir} y \emph{maxio}.
Luego de adaptado esto procedimos a las pruebas en el mismo orden que antes, versión original, versión optimizada serialmente, versión optimizada con OpenMP.

\subsection{Perfilado de aplicación con tamaño 80x80}
Previo a correr las pruebas de tiempo en el equipo PC2 realizamos un nuevo análisis de perfilado con la herramienta gprof sobre la aplicación adaptada a un tamaño distinto de problema, ya que esto puede afectar el comportamiento de las subrutinas.
Luego de compilar la aplicación con la opción ``-pg'' activada, la ejecutamos y obtenemos el archivo gmon.out de salida. Con esto podemos generar la información del perfilado, el cual indica que la subrutina \emph{estela} es la que más porcentaje del tiempo se ejecuta seguida de \emph{solgauss}, pero esta vez los porcentajes cambian completamente. La subrutina \emph{estela} se ejecuta 46,42\% del tiempo mientras que \emph{solgauss} ahora ocupa un 43,09\% (Fig. \ref{figGprof4}), esto es mucho más que el 14,36\% en PC1 o el 16,84\% en PC2 obtenido por \emph{solgauss} para la versión de 50x50.
\begin{figure}[htb]%[htp]
  \centering
  \includegraphics[width=0.60\textwidth]{figuras/gprof4.png} \\
  \caption{Salida de \emph{gprof} para tamaño 80x80.} %
   \label{figGprof4}
\end{figure}

Este cambio que se produce en la ejecución al agrandar el tamaño del problema, tendrá impacto en los tiempos de las distintas versiones de la aplicación.

\subsection{Versión serial}
La ejecución en el equipo PC2 arrojó un tiempo de ejecución de 225m43.721s, es decir 3h45m43s (Fig. \ref{figTS80}). El tamaño del problema se incrementa de 2500 paneles a 6400 paneles, un incremento de factor 2.56 veces, pero el tiempo se hace exponencial, en un factor de 9.78.

\begin{figure}[htb]
 \centering
	\begin{lstlisting}[style=consola1, numbers=none]
    real    225m43.721s
    user    174m29.803s
    sys     3m11.953s
    live@PC1 $ 
	\end{lstlisting}
\caption{Tiempo de la versión original con tamaño 80x80.}
\label{figTS80}
\end{figure}

El espacio en disco utilizado fue de 4415MB o 4.3GB, siendo los archivos ``.tmp'' los que ocupaban 4375MB, siete de los ocho archivos pesando 625MB cada uno. En memoria RAM observamos que la aplicación llega a ocupar 1293MB o 1.26GB fuera de la subrutina \emph{solgauss} y 1581Mb dentro de la subrutina.

\subsection{Versión Optimizada Serialmente}
Los tiempos observados en la versión optimizada del código serial son de 150m45.602s, es decir 2h30m45s (Fig. \ref{figTI80}). El factor de incremento esta vez es de 8.83 con respecto a la aplicación equivalente en el problema de menor tamaño. Por esto se observa una ganancia de tiempo con respecto a la aplicación serial de 75m aproximadamente, o un factor de 1.5, el cual es mejor que ante el problema de menor tamaño. Esto puede ser adjudicado a la mayor cantidad de datos en disco que utiliza la aplicación con este tamaño de problema, en comparación al tamaño 50x50, que ahora son accedidos en memoria. 

\begin{figure}[htb]
 \centering
	\begin{lstlisting}[style=consola1,numbers=none]
    real    150m45.602s
    user    150m36.178s
    sys     0m3.413s
    live@PC1 $ 
	\end{lstlisting}
\caption{Tiempo de la versión optimizada serialmente con tamaño 80x80.}
\label{figTI80}
\end{figure}

En disco se ve claramente el impacto de no utilizar los archivos ``.tmp'' al ocupar solo 40MB.

El incremento, como en la versión de menor tamaño, se ve en la memoria. Observamos que en ejecución la aplicación utiliza mientras está en \emph{solgauss} 3483MB (3.4GB) y 3171 (3.09GB) en el resto de la ejecución. El equipo cuenta con 6GB de memoria RAM por lo que no fue necesario que realizara intercambio hacia disco (swapping), lo que hubiera impactado en los tiempos.    

\subsection{Versión con optimización paralela}
Los tiempos obtenidos en la versión con OpenMP son de 130m39.169s (Fig. \ref{figTOmp80}) o 2h10m39s. Se puede observar en las versiones optimizadas, principalmente por el perfilado con gprof ya mencionado y también siguiendo la salida que da el programa por pantalla, que la demora ahora se ubica en la subrutina \emph{solgauss}. 

\begin{figure}[htb]
 \centering
	\begin{lstlisting}[style=consola1, numbers=none]
    real    130m39.169s
    user    253m34.730s
    sys     0m15.825s
    live@PC1 $ 
	\end{lstlisting}
\caption{Tiempo de la versión optimizada paralelamente con tamaño 80x80.}
\label{figTOmp80}
\end{figure}

El tiempo obtenido nos da una mejora que no es igual a la observada en la versión de 50x50, esta vez representa solo una mejora en un factor de 1.73 sobre la aplicación original.

Si analizamos el tiempo teniendo en cuenta el resultado de gprof para este tamaño de problema (Fig. \ref{figGprof4}) y para gprof para el tamaño menor (Fig. \ref{figGprof1} y \ref{figGprof2}) podemos ver que la paralelización impacta sobre un 30\% menos de tiempo, limitando la mejora obtenida al incrementar el tamaño del problema. Esto ocurre porque sólo se paraleliza la subrutina \emph{estela}, siendo que la subrutina \emph{solgauss} ahora consume ese 30\% de tiempo. La paralelización de \emph{solgauss} es un posible trabajo futuro.

El comportamiento en disco es exactamente el mismo que en la versión optimizada serialmente, con 40MB de archivos. 
En memoria ocurre igual que en la aplicación con tamaño 50x50, ocupando menos que en la versión optimizada serialmente, 3184MB (3.1GB) mientras está en \emph{solgauss} y 2864MB (2.79GB) en el resto de la ejecución.

El resumen de los datos totales obtenidos con el tamaño de problema de 80x80 pueden verse en la Fig. \ref{figDataTotal80}.

\begin{figure}[htb]%[htp]
  \centering
  \includegraphics[width=1.0\textwidth]{figuras/tabla-data80-final1.png} \\
  \caption{Resumen datos totales de ejecución 80x80.} 
   \label{figDataTotal80}
\end{figure}
\end{comment}
\section{Conclusión}
En este capítulo hemos presentado distintas pruebas de ejecución de la aplicación bajo estudio durante el proceso de su optimización, distinguiendo tres etapas: aplicación original, aplicación optimizada serialmente y aplicación optimizada paralelamente. 
Además se utilizaron dos plataformas de hardware distintas para dar mayor amplitud a la prueba y poder observar el comportamiento de la aplicación con distinto hardware. 
También se realizó una prueba con un tamaño de problema mayor para ver el impacto de la paralelización y se pudo ver el impacto en la memoria RAM, además de encontrar un perfilado distinto que en la versión de tamaño de problema menor.\\
Se han podido tomar mediciones de tiempo y de recursos para presentar conclusiones en el siguiente capítulo del trabajo realizado.

Para finalizar se puede afirmar que la aplicación desde su versión original hasta la versión optimizada y paralelizada resultante de este trabajo de tesis, ha obtenido una mejora en su velocidad de ejecución en un factor de 1.73 en el tamaño de problema mayor (80x80 paneles), y entre 2.59 y 3.58 en su versión de menor tamaño (50x50 paneles).

